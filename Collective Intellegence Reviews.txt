---- Programming Collective Intelligence ----


---- Chapter 2: Making Recommendations ----

- 2.1	Collaborative Filtering
	- 相似度评价值: Distance = 1 - Similarity
		- Eculid Distance: 1 / (1 + sqrt(sum of squares))
		- Pearson Correlation: (n*sum(xy) - sum(x)sum(y))/(sqrt(n*sum(x^2)-(sum(x))^2)*sqrt(n*sum(y^2)-(sum(y))^2))
		- Jaccard Distance
		- Manhatten Distance

- 2.2	Item-Item vs. User-User

- 2.3	使用网站API下载数据: pydelicious

- 2.4	使用MovieLens的数据集: www.grouplens.org

--------------------------------------------------------------------------------------

---- Chapter 3: Data Clustering ----

- 3.1	使用Universal Feed Parser解析RSS订阅源数据: (import feedparser)
	- d = feedparser.parser(url)
	- d.entries -> .title/.summary/.description

- 3.2 分级聚类算法:
	- Initialization: 数据集的每一行单独各为一类
	- while clusters > 1:
		- Find minimal distance: 遍历每一个配对, 寻找最小距离的配对
		- Update: 计算两个聚类的平均值, 合并形成新的类

- 3.3 使用Python Imaging Library(PIL)绘制图形: import Image, ImageDraw
	- img = Image.new('RGB', (W, H), (R, G, B))
	- draw = ImageDraw.Draw(img)
	- draw.line((x1, y1, x2, y2), fill=(R, G, B))
	- draw.text((x, y), string, fill=(R, G, B))
	- img.save(jpeg, 'JPEG')

- 3.4 K-均值聚类算法
	- 确定每个点的最小值和最大值
	- 随机创造k个中心点
	- Iterate n times until converged:
		- 在每一行中寻找距离最近的某个中心点
		- 如果结果与上次相同, break
		- 把中心点移到其所有成员的中心点

- 3.5 使用Beautiful Soup解析网页、构造结构化数据: import urllib2, from BeautifulSoup import BeautifulSoup
	- c = urllib2.urlopen(URL)
	- soup = BeautifulSoup(c.read())
	- links = soup(tag) // e.g. tag = 'a', 'td' ... 

- 3.6 多维缩放: 利用二维形式展现数据
	- 记录每一对数据项之间的真实距离
	- 随机初始化节点在二维平面中的起始位置
	- Iterate m times:
		- 计算投影距离: sqrt(sum of squares)
		- 移动节点: 对于每对pair
			- 计算误差值: (投影距离-真实距离)/真实距离
			- 计算节点移动值: 每个节点根据误差值大小, 按比例远离或靠近其他节点
			- 计算总误差值
		- 如果节点移动后总误差值变大, 结束循环
		- 根据节点移动值和设定移动比率(rate=0.01), 移动每个节点的位置

--------------------------------------------------------------------------------------

---- Chapter 4: Search Engine: Search And Ranking ----

- 4.1	使用urllib2和Beautiful Soup编写网络爬虫
	- Crawler Class:
		- Initializer: 建立数据库
		- Destructer: 关闭数据库
		- getEntryID(helper): 获取条目的ID, 如果条目不存在就将其加入数据库
		- addToIndex: 为每个网页建立数据库的索引
		- getTextOnly: 从一个HTML网页中提取文字
		- separateWords: 对字符串进行分词处理
		- isIndexed: 判断url是否已经建立过索引
		- addLinkReference: 添加一个关于两个网页的链接
		- crawl: 对一组pages进行BFS搜索, 为网页建立数据库索引, 直到给定深度
		- createIndexTables: 创立数据库表格

- 4.2	使用pysqlite建立和使用数据库: from pysqlite2 import dbapi2 as pysqlite
	- self.con = sqlite.connect(dbname)
	- self.con.execute(SQL)
		- 'create table tablename(args)'
		- 'create index indexname on tablename(args)'
		- 'insert into tablename(args) values (argvalues)'
		- 'select arg from tablename where arg=value'
		- 'select * from tablename where arg=value'
	- .fetchone()
	- self.con.commit()
	- self.con.close()

- 4.3	基于内容的排名:
	- 归一化函数(0~1): 1) Small is Better: minvalue/value 2) Large is Better: value/maxvalue
	- 单词出现频率: 频率越高越重要
	- 单词在文档中的位置前后: 越靠前越重要
	- 单词距离加和
	- 外部回指链接

- 4.4 PageRank算法:
	- Initialize with random values or 1.0
	- PageRank(to) = 0.15 + 0.85(sum(PageRank(from)/links(from)))

- 4.5	人工神经网络: Input Layer -> Hidden Layers -> Output Layer
	- searchnet类:
		- initializer: 建立数据库
		- destructor: 关闭数据库
		- maketables: 新建数据库表格
		- getStrength: 从数据库中获得链接强度(fromid, toid, layer, strength)
		- setStrength: 在数据库中设置链接强度(fromid, toid, layer, strength)
		- generateHiddenNodes: 建立一层Hidden Layer, 存from->hidden和hidden->to的链接强度
		- getAllHiddenNodes: 得到隐藏层数据
		- feedForward: 前馈算法得到输出层结果
		- backPropogate: 反向传播训练
		- trainQuery: 利用training data训练神经网络
		- updateDatabase: 更新数据库信息

	- Feed Forward前馈算法: Vector1 -> Matrix -> Vector2, S函数
		- Vector2[j] = S(sum(Vector1[i] * Matrix[i][j] for all i's))
		- S函数可选择tanh(x)或sigmoid(x)

	- Back Propogation反向传播算法: 需要反向S函数 dtanh(y): 1.0-y*y
		- 对于Input Layer中每个节点:
			- 计算输出结果与真实结果的误差
			- 根据反S函数决定每个节点总输入值的变化
			- 确定链接强度的改变值(即输入权重), 需要预先设定好learning rate

	- Traning Query:
		- If necessary, generate hidden node
		- setup neural network
		- feed forward
		- get targets
		- back propogate using targets
		- update database

--------------------------------------------------------------------------------------

---- Chapter 5: Optimization ----

- 5.1	描述问题
	- 题解:
		- example1: solution = [人序号 -> 航班index]
		- example2: solution = [宿舍序号 -> 人index]
	- 成本函数:
		- example1: cost = 价格 + 旅行时间 + 等待时间*0.5 + 汽车租用费
		- example2: cost = 最优选择*0 + 次优选择*1 + 非偏好选择*3

- 5.2	优化算法
	- 随机优化(Baseline):
		for n iterations:
			solution = random points
			find the best solution

	- Hill Climbing算法:
		- Initialize with a random solution
		- while True:
			- 创造相邻解的列表
			- 在相邻解中寻找最优解
			- 如果没有更好解, break

	- Random Start Hill Climb算法
		－ 运行Hill Climbing算法多次, 寻找其中的最优解

	- 模拟退火算法:
		- Initialize with a random start
		- while Temp > threshold(0.1):
			- 随机选择一个index
			- 随机选择一个改变方向: -1, 0, +1
			- 计算新solution
			- 计算新cost
			- if 新cost < 原cost 或者 random < e^(-(新cost-原cost)/T): 更新solution
			- 降低温度 T *= cooling rate

	- 遗传算法: 预先设定Population size, Mutation Probability and Elite Portion
		- 构造初始种群: Randomly initialize
		- 胜出者数量: Elite Portion * Population size
		- Iterate m times:
			- 按照cost排序, 选出胜出者组成新种群
			- while 新种群数量 < 原种群数量:
				- P(Mutation): 随机一个变异 -> random i, sol[:i] + mutated i + sol[i+1:]
				- 1 - P(Mutation): 随机一对杂交 -> random i, sol1[:i]+sol2[i:]
		- 第一个解即为最优解

- 5.3	使用minidom提取XML信息: import xml.dom.minidom
	- dom = xml.dom.minidom.parseString(urllib2.urlopen(URL).read())
	- r = dom.getElementsByTagName(name)
	- node = r[0].firstChild
	- unicodeString = r[0].firstChild.data

- 5.4	三个优化应用范例: 航班规划、学生宿舍安排、网络可视化。
	- 判断两线段是否交叉的算法:
		den = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1)
		if den == 0: parallel or identical
		ua = ((x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3)) / den
		ub = ((x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3)) / den
		if 0 < ua < 1 and 0 < ub < 1: intersected

--------------------------------------------------------------------------------------

---- Chapter 6: Document Filtering ----

- 6.1 	构建分类器:
	- Classifier Class:
		- Member variables:
			- self.fc: 统计占有(特征, 类别)的pair的count
			- self.cc: 统计某个分类的count
			- self.getFeatures: 获得特征的函数名
		- Member functions:
			- train(item, cat): 训练分类器通过数据更新self.cc和self.fc
			- fprob(f, cat): 计算特征在某分类的概率
			- weightedProb(f, cat, prob, weight, assumeProb): 假设一个权重为1的初始概率, 计算总加权平均概率
		- Helper functions:
			- incrementFeature(feature, category): 对应(特征, 类别)的count+1
			- incrementCategory(category): 对应分类的count+1
			- featureCount(feature, category): return 对应(特征, 类别)的count
			- categoryCount(category): return 对应类别的count
			- totalCount: return self.cc的总count
			- categories: return self.cc的keys作为list

- 6.2	构建Naive Bayers分类器:
	- Naivebayers Class: inherit Classifier Class
		- Member variables:
			- self.thresholds: 各个分类的threshold值, 只有negative/positive超过threshold才spam
		- Member functions:
			- docprob(item, cat): Pr(Doc/Cat), 提取特征并将所有单词的概率值相乘得到总概率
			- prob(item, cat): Pr(Doc/Cat) * Pr(Cat)
			- classfy(item): 寻找所有分类中的最大概率, 确保概率值ratio超过threshold, 返回最大值的分类
		- Helper functions:
			- setThreshold(cat, t)
			- getThreshold(cat)

- 6.3	构建Fisher Method分类器:
	- Fisher Class: inherit Classifier Class
		- Member variables:
			- self.minimums: 各个分类的valid概率最小值, 低于最小值则使用default/unknown分类
		- Member functions:
			- cprob(f, cat): 特征概率/所有特征概率和
			- fisherprob(item, cat):
				- 将所有概率值相乘
				- 将结果取自然对数并乘以-2
				- 利用inverse chi sqaure function求最终概率
		- Helper functions:
			- inv2chi(chi, df): 
				m = chi / 2.0
				sum = term = math.exp(-m)
				for i in range(1, df/2):
					term *= m / i
					sum += term
				return min(sum, 1.0)

- 6.4	使用SQL数据库: from pysqlite2 import dbapi2 as sqlite
	- self.con = sqlite.connect(database)
	- 建立fc和cc表格
	- 将Classifer Helper functions更改为对数据库的操作
	- self.con.commit()

- 6.5	过滤RSS订阅源: import feedpasrer
	- read(feed, classifer)函数:
		- parse订阅源, 遍历循环订阅源的内容
			- 将entries中的'title','publisher'和'summary'打印输出
			- 使用分类器打印当前最佳推测结果
			- 要求用户输入正确分类以训练
	- entryFeatures(entry)函数: 对特征提取加以改进
		- 用dict存储特征
		- 提取title中的单词加上"Title:"
		- 提取publisher中的单词加上"Publisher:"
		- 提取并统计summary中的单词
		- 统计大写单词, 如果大写单词数超过某个比例, 增加名为UPPERCASE的特征
		- 统计summary中词组

- 6.6	使用Akismet: import akismet
		- 在官网上申请API key
		- akismet.verify_key(apikey, pageurl)
		- akismet.comment_check(apikey, pageurl, ipaddress, agent, comment_content, comment_author_email, comment_type="comment")

--------------------------------------------------------------------------------------

---- Chapter 7: Decision Tree ----

- 7.1 	构建决策树node:
		- col: 待检验的判断条件对应的index
		- value: 标准值, 用于判断true/false
		- results: leaf上代表结果的dictionary, 如果是中间节点则一定是None
		- truebranch: True分支
		- falsebranch: False分支

- 7.2 	选择合适的拆分方案:
		- Gini Inpurity: 每个pair的出现频率的乘积总和 sum(p(i)*p(j)) (i!=j)
		- Entropy: 针对所有结果的 -p(i) * log(p(i))之和
		- Variance: 方差, 用于处理数值型数据
		- Information Gain:
			- weight1 = |subset1|/|set|
			- weight2 = |subset2|/|set|
			- IG = Entropy(set) - weight1*Entropy(subset1) - weight2*Entropy(subset2)

- 7.3	构建决策树:
		- 定义变量以记录最佳拆分条件: bestGain, bestCriteria, bestSets
		- for column in range(numOfcolumns):
			- 记录由这一列每个值构成的set
			- 根据set中每个值, 尝试对数据集进行拆分, 获得最佳拆分结果
		- 如果bestGain > 0, recursively创建两个子分支, 并返回当前node; 否则, 返回带结果的leaf node

- 7.4	决策树的显示:
		- 打印显示
		- 树状图显示

- 7.5	使用决策树预测结果:
		- 如果是leaf node则返回results
		- 获得tree对应的col的观测值
		- 如果是数值:
			- >= value: true branch
			- < value: false branch
		- 如果是非数值:
			- == value: true branch
			- != value: false branch
		- recursively预测branch

- 7.6	决策树的prune: 设置minigain
		- 如果分支不是leaf node, 则对其进行recursive prune操作
		- 如果两个子分支都是leaf node, 则判断它们是否需要合并:
			- 构造合并后results的数据集
			- 检查Entropy的减少, 如果小于minigain, 则合并两个分支

- 7.7	处理缺失数据: 和7.5一样的函数, 在获得col对应观测值后加一段代码
		- 如果对应观测值是None:
			- recursively预测两个分支
			- sum count两个分支预测结果, 并计算权重
			- 根据权重计算新的results并返回

- 7.8 	两个应用实例:
		- Example 1: 对住房价格进行建模
			- www.zillow.com API
			- 使用xml.dom.minidom
			- 函数: getAddressData, getPriceList
			- 生成决策树
		- Example 2: 对照片热度进行建模
			- HotOrNot API (已过期)
			- 函数: getRandomRatings, getPeopleData
			－ 生成决策树

- 7.9	什么使用应该使用决策树:
		- 优势: 
			- 容易对模型进行解释
			- 选择有关的数据
			- 可以同时接受分类型和数值型的数据
			- 允许数据的不确定性分配
		- 缺点: 
			- Overfitting
			- 对拥有大量可能结果的数据集时会变的异常复杂
			- 数值型数据只能通过大小关系分类

--------------------------------------------------------------------------------------

---- Chapter 8: kNN Algorithm ----

- 8.1 	kNN算法:
		- 定义相似度:
			- Eculidean distance
			- Jaccard distance
			- Manhatten distance
		- kNN(data, vec, k):
			- 构造data中每个元素对vec的距离的数组[(distance, index)]
			- 对数组依据distance从小到大排序
			- 获得排序后的数组前k项的index
			- 从data中获得index对应的result
			- 对所有result求平均值, 即为预测结果

- 8.2 	分配权重的kNN算法:
		- 分配权重函数:
			- Inverse function: y = 1/(x+0.1)
			- Substraction function: y = max(1 - x, 0)
			- Gaussian(sigma): y = exp(-x**2/(2*sigma**2))
		- 加权kNN算法:
			- 得到从小到大依照距离排序的数组[(distance, index)]
			- 获得排序后的数组前k项的distance, index, result
			- 由前k项distance获得前k项的权重
			- 获得总权重
			- 对result求加权平均值, 即为预测结果

- 8.3	交叉验证:
		- 拆分数据集:
			- 确定testset的百分比, e.g. 0.05
			- 遍历整个数据集, 根据随机数是否小于这个百分比将数据放入testset或trainset
		－ 交叉验证(trails):
			- 在trails次循环中:
				- 拆分数据集
				- 计算对testset的variance error
			- 计算平均总error

- 8.4	对数据集进行缩放优化:
		- 对数据集进行缩放:
			- 加入scale = [x1, x2, ...]
			- 对每个维度乘以相应的scale值
		- 缩放优化
			- 利用优化算法找出最佳scale
			- 确定scale的domain
			- 成本函数: 交叉验证函数
			- 优化算法: 模拟退火, 遗传算法

- 8.5 	不对称的分布:
		- 估算概率密度:
			- input range from low to high
			- count 处于range内的数据权重
			- count 总数据权重
			- 概率密度 = 处于range内的数据权重和/总数据权重和
		- 绘制概率分布图(使用matplotlib):
			- t1 = pylab.arange(min, max, step)
			- 获得probs
			- 对概率进行平滑处理, 获得smoothed
			- pylab.plot(t1, smoothed)
			- pylab.show()

- 8.6	使用ebay API: import httplib
		- 使用xml建立连接:
			- connection = httplib.HTTPSConnection(serverURL)
			- connection.request('POST', '/ws/api.dll', xml, getHeaders(apicall))
			- response = connection.getresponse()
			- if response.status != 200: error
			- else: data = response.read()
			- connection.close()
		- 使用ebay API:
			- finding API: 直接使用url
			- trading API: 使用xml建立连接

--------------------------------------------------------------------------------------

---- Chapter 9: Kernel Methods & SVM ----			

- 9.1 	构建数据集:
		- row:
			- .data = [x1, x2, ...]
			- .match = y (-1 or +1)
		- plot:
			- pylab.plot(x, y, 'go')
			- pylab.show()
		- 将数据集构造成数值型float, yesOrNo and distance
		- 对数据进行缩放处理:
			newdata[i] = (data[i] - min_data[i])/(max_data[i] - min_data[i])

- 9.2 	利用Google Maps API获取地理位置和距离:
		- Geocoding API:
			- url = 'http://maps.googleapis.com/maps/api/geocode/xml?address=' + address.replace(' ', '+')
			- data = urllib2.urlopen(url).read()
			- doc = xml.dom.minidom.parseString(data)
			- lat = doc.getElementsByTagName('lat')[0].firstChild.nodeValue
			- lng = doc.getElementsByTagName('lng')[0].firstChild.nodeValue 	
			- time.sleep(0.2) # If request too frequetly, the service will stop
		- 使用Haversine公式计算距离:
			- haversineDistance(lat1, lng1, lat2, lng2, miles=True):
				  R = 3959.0 if miles else 6371.0
				  phi1 = math.radians(lat1)
				  phi2 = math.radians(lat2)
				  deltaphi = math.radians(lat2 - lat1)
				  deltalambda = math.radians(lng2 - lng1)
				  a = math.sin(deltaphi/2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(deltalambda/2)**2
				  c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
				  return R * c

- 9.3	核方法:
		- 意义:
			- 对于不能够线性分割的数据集, 可以将其从低维空间转变到高维空间, 从而使数据集可以线性分割
			- 核方法使得这种维度变换不需要显式进行, 而直接返回数据集在高维空间的结果
		- RBF(gamma):
			- dv = sum([(v2[i]-v1[i])**2 for i in range(len(v1))])
			- return math.exp(-dv*gamma)
		- 非线性分类器:
			- y = rbfsum0/count0 - rbfsum1/count1 + offset
			- offset = rbfsum1_1/len(l1)**2 - rbfsum0_0/len(l0)**2
			- if y > 0: return 0 else return 1

- 9.4	SVM:
		- 使用LIBSVM:
			- import svmutil
			- prob = svm_problem(y, x)
			- param = svm_parameter(setup)
			- m = svm_train(prob, param)
			- label, acc, val = svm_predict(y, x, m) # y is test targets
		- 使用scikit-learn:
			- import svm, metrics
			- classifier = svm.SVC(gamma=0.001)
			- classifier.fit(train_data, train_targets)
			- predicted = classifier.predict(test_data)
			- print metrics.classfication_report(test_targets, predicted)

--------------------------------------------------------------------------------------

---- Chapter 10: Feature Extraction ----

- 10.1 	搜索新闻订阅源数据:
		- Data structures: Allwords(dict), ArticleWords([dict]), ArticleTitles([])
		- 遍历每个数据源:
			- 用feedparser解析数据源
			- 遍历每篇文章(entry):
				- 跳过标题相同的文章
				- 提取单词: entry.title, entry.description
				- 遍历每个单词:
					- 增加Data Structure的计数
					- 当前index ＋1
		- 构造单词矩阵: (sparse matrix)
			- 每个column代表单词
			- 每个row代表文章
			- 矩阵数值代表单词在某篇文章中的出现次数

- 10.2 	Non-Negative Matrix Factorization: 自定义特征数量
		- 文章矩阵m ＝ 权重矩阵w * 特征矩阵h
		- 算法:
			- 随机初始化w和h
			- 在maxiter次循环中:
				- wh = w * h
				- 计算wh和原矩阵的平方误差和, 如果为0则break
				- hn = w.T * m, hd = w.T * w * h
				- h = h .* hn ./ hd
				- wn = m * h.T, wd = w * h * h.T
				- w = w .* wn ./ wd
			- 返回w和h

- 10.3 	呈现结果:
 		- for i in range(特征数量):
 			- col_list = [(h[i, j], columns[j]) for j in range(columns)]
 			- row_list = [(w[j, i], rows[j]) for j in range(rows)]
 			- 对list从大到小排序选出最重要的项目

- 10.4 	下载股票数据: 利用Yahoo! iChart
 		- url = 'http://ichart.finance.yahoo.com/table.csv?'s=GOOG&d=3&e=1&f=2015&g=d&a=3&b=1&c=2005&ignore=.csv'
 		- 表示Google从2005年4月2日到2015年4月2日的全部数据
 		- 读取数据: urllib2.urlopen(url).readlines()

--------------------------------------------------------------------------------------

---- Chapter 11: Genetic Programming ----

- 11.1  遗传算法的思想:
		- Step 1: 创建随机种群
		- Step 2: 对每个解进行排序
		- Step 3: 如果解足够好, 则停止运行, 否则继续运行
		- Step 4: 复制种群中前面的top解
		- Step 5: 变异/交叉
		- Step 6: 生成数量与原种群相同的新种群
		- 回到Step 2

- 11.2 	构造程序解析树:
		- 函数封装类: function wrapper
			- member variables: function, childcount, name
		- 程序结点:
			- 函数节点:
				- member variables: function, name, children
				- evaluate(inp): 计算函数结果
				- display(): 展现程序
			- 参数节点:
				- member variables: index
				- evaluate(inp): inp[index]
				- display(): 展现程序
			- 常数节点:
				- member variables: value
				- evaluate(inp): value
				- display(): 展现程序

- 11.3 	构建随机程序数: paramCount, maxdepth, funcprob, paramprob
		- if random < funcprob and maxdepth > 0: 返回函数节点
		- if random < paramprob: 返回参数节点(0~paramCount-1)
		- else: 返回常数节点(random value)

- 11.4	衡量题解优劣程度:
		- dif = 0
		- for data in dataset:
			- dif += abs(tree.evaluate(data.params) - data.result)
		- return dif

- 11.5 	程序变异: tree, paramCount, prob
		- if random < prob: 返回随机程序树
		- else: 
			- result = deepcopy(tree)
			- 如果tree是函数节点: result.children = 对每个child进行变异
			- 返回result

- 11.6 	程序交叉: tree1, tree2, prob, top
		- if random < prob and not top: return deepcopy(tree2)
		- else:
			- result = deepcopy(tree1)
			- 如果tree1和tree2都是函数节点: result.children = tree1中每个children和tree2中随机child进行交叉
			- 返回result

- 11.7	程序进化: paramCount, popsize, rankFunc, maxgen, mutationRate, breedingRate, pexp, pnew
		- def selectIndex(): return int(log(random)/log(pexp)) # pexp越大, 拣选越严格
		- 创建随机种群population
		- for i in range(maxgen):
			- scores = rankFunc(population) # 给出排序后的(score, tree)
			- if scores[0][0] == 0: break
			- newpop = 挑选top 2个tree组成的优秀种群
			- while len(newpop) < popsize:
				- if random < pnew: 加入随机节点以增加多样性
				- else:
					- 对最前2个tree进行交叉后再对结果进行变异, 加入新种群
			- population = newpop
		- return scores[0][1]

- 11.8 	Grid Game:
		- gridgame:
			- 确定游戏区域大小, 默认4x4
			- 用一个list记住每位玩家上一步
			- 用一个[[x1, y1], [x2, y2]...]标记玩家位置, 两个玩家需要距离足够远 ((loc1+2)%4)
			- for _ in range(maxsteps):
				- for 玩家i 和 对手j:
					- 用locs标记玩家i和玩家j的位置
					- locs.append(lastmove[i])
					- move = players[i].evaluate(locs) % 4
					- 如果在1行中同一方向移动2次, 对手j获胜
					- lastmove[i] = move
					- 对move是0, 1, 2, 3: 改变location[i][x,y]的值, 如果碰触bound则算不动
					- if location[i] == location[j]: 玩家i获胜
			- return -1 # 平局
		- tournament function: rank function
			- 用list losses表示每位玩家失败次数
			- for 玩家i 和 玩家j:
				- if i == j: continue
				- winner = gridgame([player i, player j])
				- if winner == 0: losses[j] += 2
				- if winner == 1: losses[i] += 2
				- if winner == -1: losses[i] += 1, losses[j] += 1
			- scores = zip(losses, players)
			- scores.sort()
			- return scores
		- humanplayer:
			- def evaluate(self, board):
				- 获得自己和其他玩家的位置(tuple)
				- 在board每个坐标:
					- 如果是自己, 则打印'O'
					- 如果是其他玩家, 则打印'X'
					- 空位置: 打印'-'
				- 获得用户move = int(raw_input())
				- return move

- 11.9	遗传编程的扩展:
		- 更多数值型函数: 三角函数, 数学函数, 统计分布, 距离度量, 多参数函数
		- 增加记忆力: 
			- memory slot: store information
			- store node: child node, index->slot
			- recall node: index->sort
			- share memory
		- 不同数据类型:
			- str: concat, split, index, substring
			- list: concat, split, index, sublist
			- dictionary: replace, add
			- object: member variables, member functions

--------------------------------------------------------------------------------------

---- Chapter 12: Summary of Algorithms ----

－ 12.1 	Naive Bayersian Classifer:
		- 优点:
			- 接受大数据量训练和查询时的高速度
			- 增量式训练/Online Learning
			- 易于解释对分类器的学习情况
		- 缺点:
			- 无法处理基于特征组合所产生的变化结果 (naive assumption fails)

- 12.2 	Decision Tree Classifier:
		- 优点:
			- 解释一个训练模型是非常容易的(不仅对分类有价值, 还可以解释决策过程)
			- 可以同时接受数值型数据和分类型数据
			- 与Bayersian相比, 容易处理变量之间的相互影响
		- 缺点:
			- 并不擅长对数值结果进行预测
			- 不支持增量型训练
			- 容易Overfitting
		- 应用范围:
			- 更适宜小规模的数据集

- 12.3 	Neural Networks:
		- 优点:
			- 能够处理负责非线性函数, 并且能够发现不同输入间的依赖关系
			- 允许增量式训练, 仅需要保留代表权重的矩阵
		- 缺点:
			- 是一种黑盒方法, 很难知道如何得到预测结果
			- 在选择训练数据的比率和控制网络大小方面, 并没有明确的规则可以遵循, 而是依赖大量的经验

- 12.4	SVM:
		- 优点:
			- 在正确的参数支持下, 执行效果很好
			- 对新的观测数据进行分类时, 速度极快
			- 同时支持分类型数据和数值型数据
		- 缺点:
			- 针对每个数据集的最佳核变换函数以及相应的参数都是不一样的
			- 黑盒方法, 难以解释预测模型
		- 应用范围:
			- 更适宜大规模的数据集

- 12.5 	kNN:
		- 优点:
			- 能够利用复杂函数进行预测, 同时又能够保留简单易懂的特点
			- 确定合理的数据缩放量不仅能够改善预测效果, 还说明了各个变量的相对重要程度
			- Online Learning
		- 缺点:
			- 要求所有训练数据都不可缺少
			- 某一项待预测的数据都必须和所有其他数据项进行比较
			- 寻找合理的缩放因子是计算量非常大的工作

- 12.6 	Clustering:
		- 主要技术:
			- Hierarchical Clustering
			- k-Means Cluatering
			- Multi-dimensional Scaling: 高维度数据 -> 二维平面显示 (相对距离～高维度距离)
			- NMF(Non-Negative Matrix Factorization): 原矩阵 ～ 权重矩阵 * 特征矩阵

- 12.7 	Optimization:
		- 确定成本函数, 以及题解的值域: 可能会有多个local minima
		- Simulated Annealing Algorithm:
			- 如果新题解成本变小, 则新题解取代旧题解; 如果新题解成本变大, 则新题解取代旧题解的概率取决于当前温度
			- 算法在执行的早期阶段会更容易接受表现相对较差的题解, 从而有效避免陷入局部最小值的可能
		- Genetic Algorithm:
			- 由随机种群开始, 选出表现最好/成本最低的成员, 并通过变异/交叉获得新成员以达到原有种群规模。

- 12.8 	第三方函数库:
		- Universal Feed Parser: import feedparser
			- feed = feedparser.parse(url)
		- PIL: from PIL import Image, ImageDraw
			- img = Image.new('RGB', (w, h), (255, 255, 255))
			- draw = ImageDraw.Draw(img)
			- draw.line((x1, y1, x2, y2), fill=(R, G, B))
			- draw.text((x, y), text, fill=(R, G, B))
			- img.save(filename, 'JPEG')
		- BeautifulSoup: from BeautifulSoup import BeautifulSoup
			- soup = BeautifulSoup(urllib2.urlopen(url).read())
		- Pysqlite: from pysqlite2 import dbapi2 as sqlite
			- con = sqlite.connect(databasename)
			- con.execute('SQL')
			- con.commit()
			- con.close()
		- NumPy: import numpy
		- Matplotlib: import pylab
			- pylab.plot([x], [y], 'ro')
			- pylab.savefig(filename)
			- pllab.show()
		- Pydelicious: import pydelicious
			- pydelicious.get_popular(tag='programming')
			- pydelicious.get_userposts('jinfeng')
			- a = pydelicious.apiNew(user, password)
			- a.posts_add(url=url, description='', extended='', tags='')

- 12.9	一些重要的数学概念:
		- Eculidean Distance
		- Pearson Correlation Coefficient
		- Weighted Mean
		- Jaccard/Tanimoto Coefficient
		- Conditional Probability
		- Gini Inpurity
		- Entropy
		- Variance
		- Gaussian Function
		- Dot products


 



















